{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf901a5-1716-4398-95bc-6a4fefd0aa8c",
   "metadata": {},
   "source": [
    "# FINK Portal Database Query for Crossmatched Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e68a5-69cf-4fcf-bc5c-992947ff7a32",
   "metadata": {},
   "source": [
    "Since I've been able to download the full FINK data from the crossmatched catalogue using batching before, I no longer need to query it individually within each notebook. To save on space and having to redo it for each notebook, I've created this notebook to house the code necessary to query the portal and save the resulting dataframe as a csv file to be imported into the other notebooks.\n",
    "\n",
    "Please note that the Kernel may die if you're batching lots of sources, and that you will need to find the index at which the request failed and continue the batching proccess from there. Simillarly, the kernel may crash if you try to append each batch into a single dataframe to save it as an csv file. In this case, you will have to make to with grouping the batches as best you can, then calling those groups to perform whatever analysis you're trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38cce5d7-7011-4858-9d32-2a99c0c6616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here are the necessary imports\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from io import StringIO\n",
    "from vasttools.pipeline import Pipeline\n",
    "from vasttools.query import Query\n",
    "import Projecttools as pro #brand new module for frequently used code!\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8a6db3-2676-4d98-8bcb-578378d502ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This reads in the crossmatch catalogue\n",
    "cms = pd.read_pickle('Fink_2020_sources_matched_to_VAST_all_sources.pickle')\n",
    "\n",
    "#These are the FINK IDs that are selected from the catalogue.\n",
    "Idlist=cms['objectId'].reset_index()\n",
    "Idlist.drop('index', inplace=True, axis=1)\n",
    "Idlist=Idlist['objectId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2846f3b3-1902-4958-a986-2a0b0493a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elem=len(Idlist)#length of id list\n",
    "num_chunks=num_elem//30+1 #number of chunks, based on how you want to divide them up. in this case, 30 IDS per chunk, as thats the limit of the FINK Portal query load\n",
    "list_chunks=(np.array_split(np.arange(num_elem), num_chunks))#np.arange(num_elem) makes an ordered array, from 0 to (num_elem - 1).\n",
    "                                                            #np.array_split splits said ordered array according to the number of chunks specified by num_chunks\n",
    "                                                            #each chunk is an element in the array 'list_chunks'\n",
    "\n",
    "#this just converts each chunk in list_chunks into an actual list\n",
    "for i in list(range(len(list_chunks))):\n",
    "    list_chunks[i]=(list_chunks[i]).tolist()\n",
    "\n",
    "#defining column array for cutouts\n",
    "cutouts=[\n",
    "'b:cutoutScience_stampData',\n",
    "'b:cutoutTemplate_stampData',\n",
    "'b:cutoutDifference_stampData'\n",
    "]\n",
    "\n",
    "for chunk_idx in list_chunks: #for each chunk in list_chunks\n",
    "    start,end=chunk_idx[0],chunk_idx[-1]+1 #define the starting and ending indexes for the given chunk\n",
    "\n",
    "    #this is the request made to the fink portal to pull out the info for each source\n",
    "    r = []\n",
    "    r = requests.post(\n",
    "        'https://fink-portal.org/api/v1/objects',\n",
    "        json={\n",
    "        'objectId': ','.join(Idlist[start:end]), #This is where the 'chunk_idx[-1] +1' comes into play. the 'end' variable when slicing the list is inclusive of the index.\n",
    "        'output-format': 'json',\n",
    "        'withcutouts': 'True',\n",
    "        'cols': ','.join(cutouts),\n",
    "        'withupperlim': 'True' #important for lightcurve plotting\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df_tmp=[]\n",
    "    df_tmp=pd.read_json(StringIO(r.content.decode()))#define a temporary dataframe that holds the queried sources from the chunk\n",
    "    \n",
    "    #saves the temporary dataframe to a folder as a .csv file. the naming is based on which batch we're looking at\n",
    "    df_tmp.to_csv('/home/jovyan/work/Project_VAST_FINK/FINK_Batches_CSV/Batch_{}.csv'.format(list_chunks.index(chunk_idx)))\n",
    "    \n",
    "    #clears memory from jupyter to help it not get stuck.\n",
    "    gc.collect()\n",
    "    \n",
    "list_df=[] #empty array to hold fink sources.\n",
    "\n",
    "#these are the specific columns that are important to this project.\n",
    "columns = ['i:objectId',\n",
    "           'b:cutoutDifference_stampData',\n",
    "           'b:cutoutScience_stampData',\n",
    "           'b:cutoutTemplate_stampData',\n",
    "           'i:fid',\n",
    "           'd:tag',\n",
    "           'i:jd',\n",
    "           'i:magpsf',\n",
    "           'i:sigmapsf',\n",
    "           'i:diffmaglim',]\n",
    "\n",
    "#now, we're loading back in all the batches we saved and appending/concatonating them all back together into one dataframe: fsd\n",
    "for chunk_idx in list_chunks:\n",
    "    df_tmp=pd.read_csv('/home/jovyan/work/Project_VAST_FINK/FINK_Batches_CSV/Batch_{}.csv'.format(list_chunks.index(chunk_idx)), usecols = columns)\n",
    "    list_df.append(df_tmp)\n",
    "fsd=pd.concat(list_df)\n",
    "\n",
    "fsd.to_csv('/home/jovyan/work/Project_VAST_FINK/FINK_Source_Data/FSD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0542c3-a7a3-444f-920a-10c1b11aa119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
